{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import models as vision_models\n",
    "import timm\n",
    "\n",
    "\n",
    "# Зафиксируем сиды, чтобы обучение было воспроизводимым.\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все хорошо, установлена версия с поддержкой видеокарт\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Все хорошо, установлена версия с поддержкой видеокарт\")\n",
    "else:\n",
    "    print(\"Что-то не так, стоит torch с поддержкой только CPU (если у вас MacOS или так и задумано, то все нормально.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160, 5) (240, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "data = pd.read_csv(r\"D:\\data science\\T1_hol\\dataset\\annotations\\train_labels.csv\")\n",
    "le1 = LabelEncoder()\n",
    "data['image_name'] = data['image_id']\n",
    "data['unified_class'] = data['class']\n",
    "data['class_id'] = le1.fit_transform(data['class'])\n",
    "data['class_id'] = data['class_id'].astype('int64')\n",
    "images_path = r\"D:\\data science\\T1_hol\\dataset\\train\"\n",
    "\n",
    "# Разобъем данные на тренировочную и отложенную (на которой мы будем проверять качество работы) части\n",
    "train, val = train_test_split(data, test_size=0.1, random_state=1, stratify=data['class'])\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "\n",
    "# Получилось примерно 28.1к картинок для тренировки и 12.0к картинок для подсчета метрик\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class</th>\n",
       "      <th>image_name</th>\n",
       "      <th>unified_class</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_95274.jpg</td>\n",
       "      <td>sunflower</td>\n",
       "      <td>image_95274.jpg</td>\n",
       "      <td>sunflower</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_48791.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_48791.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_26601.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_26601.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_10488.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_10488.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_64214.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_64214.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>image_26027.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_26027.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>image_21087.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_21087.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>image_32962.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_32962.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>image_21896.jpg</td>\n",
       "      <td>cactus</td>\n",
       "      <td>image_21896.jpg</td>\n",
       "      <td>cactus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>image_03441.jpg</td>\n",
       "      <td>cactus</td>\n",
       "      <td>image_03441.jpg</td>\n",
       "      <td>cactus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id      class       image_name unified_class  class_id\n",
       "0     image_95274.jpg  sunflower  image_95274.jpg     sunflower         3\n",
       "1     image_48791.jpg       rose  image_48791.jpg          rose         2\n",
       "2     image_26601.jpg       rose  image_26601.jpg          rose         2\n",
       "3     image_10488.jpg       rose  image_10488.jpg          rose         2\n",
       "4     image_64214.jpg       rose  image_64214.jpg          rose         2\n",
       "...               ...        ...              ...           ...       ...\n",
       "2155  image_26027.jpg       rose  image_26027.jpg          rose         2\n",
       "2156  image_21087.jpg       rose  image_21087.jpg          rose         2\n",
       "2157  image_32962.jpg       rose  image_32962.jpg          rose         2\n",
       "2158  image_21896.jpg     cactus  image_21896.jpg        cactus         0\n",
       "2159  image_03441.jpg     cactus  image_03441.jpg        cactus         0\n",
       "\n",
       "[2160 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val.drop(['class','image_id'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.model = vision_models.efficientnet_b1(vision_models.EfficientNet_B1_Weights.DEFAULT)\n",
    "        self.model.classifier[1] = torch.nn.Linear(self.model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inputs, _ = batch\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\timm\\models\\_factory.py:117: UserWarning: Mapping deprecated model name swin_base_patch4_window7_224_in22k to current swin_base_patch4_window7_224.ms_in22k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa6e92c2fb346788fd6e8dabd451f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\danie\\.cache\\huggingface\\hub\\models--timm--swin_base_patch4_window7_224.ms_in22k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, num_classes: int, model_name: str = 'swin_base_patch4_window7_224', pretrained: bool = True):\n",
    "        \"\"\"\n",
    "        Initializes a Swin Transformer model.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of output classes.\n",
    "            model_name (str): The name of the Swin Transformer model to use from timm.\n",
    "                              Defaults to 'swin_base_patch4_window7_224'.\n",
    "            pretrained (bool): Whether to use pretrained weights. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            batch: A tuple containing the input tensor and labels (optional).\n",
    "                   The input tensor should have shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            The output logits of the model.\n",
    "        \"\"\"\n",
    "        if isinstance(batch, tuple):\n",
    "            inputs, _ = batch\n",
    "        else:\n",
    "            inputs = batch\n",
    "        return self.model(inputs)\n",
    "\n",
    "# Example Usage:\n",
    "num_classes = 10  # Replace with your actual number of classes\n",
    "model = SwinTransformer(num_classes, model_name=\"swin_base_patch4_window7_224_in22k\")\n",
    "\n",
    "# Example input (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Get the output logits\n",
    "output_logits = model(dummy_input)\n",
    "print(output_logits.shape)  # Expected output: (1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models as vision_models\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.model = vision_models.resnet50(weights=vision_models.ResNet50_Weights.DEFAULT)\n",
    "        # Заменяем последний полностью связанный слой на новый с нужным количеством классов\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inputs, _ = batch\n",
    "        return self.model(inputs)\n",
    "\n",
    "# Пример использования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    def __init__(self, num_classes: int, model_name: str = 'convnext_tiny', pretrained: bool = True):\n",
    "        \"\"\"\n",
    "        Initializes a ConvNeXt model.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of output classes.\n",
    "            model_name (str): The name of the ConvNeXt model to use from timm.\n",
    "                              Defaults to 'convnext_tiny'.\n",
    "            pretrained (bool): Whether to use pretrained weights. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            batch: A tuple containing the input tensor and labels (optional).\n",
    "                   The input tensor should have shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            The output logits of the model.\n",
    "        \"\"\"\n",
    "        inputs, _ = batch\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class</th>\n",
       "      <th>image_name</th>\n",
       "      <th>unified_class</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_95274.jpg</td>\n",
       "      <td>sunflower</td>\n",
       "      <td>image_95274.jpg</td>\n",
       "      <td>sunflower</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_48791.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_48791.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_26601.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_26601.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_10488.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_10488.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_64214.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_64214.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>image_26027.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_26027.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>image_21087.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_21087.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>image_32962.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>image_32962.jpg</td>\n",
       "      <td>rose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>image_21896.jpg</td>\n",
       "      <td>cactus</td>\n",
       "      <td>image_21896.jpg</td>\n",
       "      <td>cactus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>image_03441.jpg</td>\n",
       "      <td>cactus</td>\n",
       "      <td>image_03441.jpg</td>\n",
       "      <td>cactus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id      class       image_name unified_class  class_id\n",
       "0     image_95274.jpg  sunflower  image_95274.jpg     sunflower         3\n",
       "1     image_48791.jpg       rose  image_48791.jpg          rose         2\n",
       "2     image_26601.jpg       rose  image_26601.jpg          rose         2\n",
       "3     image_10488.jpg       rose  image_10488.jpg          rose         2\n",
       "4     image_64214.jpg       rose  image_64214.jpg          rose         2\n",
       "...               ...        ...              ...           ...       ...\n",
       "2155  image_26027.jpg       rose  image_26027.jpg          rose         2\n",
       "2156  image_21087.jpg       rose  image_21087.jpg          rose         2\n",
       "2157  image_32962.jpg       rose  image_32962.jpg          rose         2\n",
       "2158  image_21896.jpg     cactus  image_21896.jpg        cactus         0\n",
       "2159  image_03441.jpg     cactus  image_03441.jpg        cactus         0\n",
       "\n",
       "[2160 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalsDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, path_to_images: Path, transforms: tt.Compose) -> None:\n",
    "        self.df = dataframe\n",
    "        self.path_to_images = path_to_images\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # print(row)\n",
    "        image = Image.open(self.path_to_images + '/' + row[\"image_name\"]).convert('RGB')\n",
    "        # print(image)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        return image, row[\"class_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import decode_image\n",
    "from pathlib import Path\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "idx = 1\n",
    "img = read_image(images_path +'/'+ data.iloc[idx][\"image_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем агументации. В нашем примере я не использую ничего, кроме изменения разрешения изображения в квадрат 224 на 224 пикселя\n",
    "# и нормализации (перевода пикселей от диапазона 0-255 к распределению с заданными средним и дисперсией).\n",
    "\n",
    "train_transform = tt.Compose([\n",
    "    # tt.RandomGrayscale(p=0.5),\n",
    "    # tt.RandomHorizontalFlip(0.5),\n",
    "    tt.RandomRotation((-5, 5)),\n",
    "    tt.Resize((224, 224)),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = tt.Compose([\n",
    "    tt.Resize((224, 224)),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = AnimalsDataset(train, images_path, transforms=train_transform)\n",
    "val_dataset = AnimalsDataset(val, images_path, transforms=val_transform)\n",
    "\n",
    "\n",
    "# Обратите внимание на\n",
    "# num_workers - во сколько отдельных потоков мы будем готовить данные (вызывать AnimalsDataset), можете поставить своё значение, посмотрев количество ядер на вашем компьютере\n",
    "# batch_size - сколько картинок одновременно мы подадим на вход нашей модели.\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, num_workers=0, shuffle=True)\n",
    "# valid_dataloader = DataLoader(val_dataset, batch_size=64, num_workers=0, shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64,  shuffle=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 4, 0, 2, 0, 2, 4, 1, 1, 1, 4, 1, 2, 0, 3, 4, 1, 1, 3, 4, 0, 4, 1,\n",
       "        0, 0, 4, 0, 2, 0, 2, 3, 1, 4, 3, 2, 2, 1, 4, 2, 2, 0, 1, 4, 2, 4, 2, 2,\n",
       "        3, 1, 4, 0, 3, 4, 3, 0, 3, 1, 3, 4, 4, 1, 2, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для обучения выбран девайс cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Для обучения выбран девайс {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7a6dcf779949cca6d0cf6621babcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62e4c082b634b5da16f39282875b2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with F1: 0.9626\n",
      "Epoch [1/50], Val Loss: 0.1420, Val F1: 0.9626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe63d38125a94973ade38c00f8685583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770dfd76e92341f9a6fb6d6859c0310e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Val Loss: 0.2880, Val F1: 0.9008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae114980377463bbbfc2f5b93fe247e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed35eee534694c3a82205d652fb27a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with F1: 0.9669\n",
      "Epoch [3/50], Val Loss: 0.1371, Val F1: 0.9669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc437faf4294f3caef1554b03a5c22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c549de889923420f94f7f186ff310cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Val Loss: 0.1462, Val F1: 0.9498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808ece6de25945c18cfcba15b3ac7295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099d5306d90647aeb6104285d2ad719a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Val Loss: 0.1911, Val F1: 0.9424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9ff7a4cace4486ba47429cc5b57e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a98856fe7e74f5bb788923228109c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with F1: 0.9709\n",
      "Epoch [6/50], Val Loss: 0.0802, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4136158e4a8246a2901821556065582f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a3420a1f454b7f824dd2cc74add8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Val Loss: 0.0711, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3868850851384bd8962690107c7da853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a5691736444edead205a1e46dfeaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Val Loss: 0.0749, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d0c47bbc4549ffa5e3b8ca8542759b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad57d0e949e411abe7b3aa45d20c669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Val Loss: 0.0824, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55eef454b86e4ccfbe185db661d29342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25a762ec38147dda5b90fcf517df405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Val Loss: 0.0735, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e68eb3a1fb434fa948bad8fca95291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487e8a1c6c874bbfb54329ffe77dc87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Val Loss: 0.0735, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d00821cc3c4ba3b7ed6fec5cc862f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc84cc053b64293ace0083305918c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Val Loss: 0.0734, Val F1: 0.9667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d244824706494c9ba0bf28380e90f162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d66120ac3604ac1b1c6862943baccd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Val Loss: 0.0732, Val F1: 0.9667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926523329647491da3f8c2f5b82d57bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70108f72a1304cd089608eb645a649be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Val Loss: 0.0736, Val F1: 0.9667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3069af061d45a08ebc74c9d841cc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a1b2c25d064e1b865f563aa9f98109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Val Loss: 0.0741, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6782c29d6a89476385e097036c647226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5721ae23fff0455ca2ba7c3cd10935cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Val Loss: 0.0741, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fa585af68249fcaa1085d7b18845d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9338a9e837e4aa193be6591ee20c800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Val Loss: 0.0741, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ec456817974220adb0ee8e4aaf24a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9fd8f5cb9f466da4c42726e9d74002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Val Loss: 0.0741, Val F1: 0.9709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53e7227132b46fbaa3123c72dc4c98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     44\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 46\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# train_true.extend(labels.cpu().numpy())\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# train_pred.extend(preds.cpu().numpy())\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# train_f1 = f1_score(train_true, train_pred, average='macro')\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# train_losses.append(running_loss / len(train_dataloader))\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# train_f1_scores.append(train_f1)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Напишем код для обучения нашей нейронной сети:\n",
    "# model = ResNet18(num_classes=data[\"unified_class\"].nunique()).to(device)\n",
    "model = SwinTransformer(num_classes=data[\"unified_class\"].nunique()).to(device)\n",
    "# model = Megadescriptor(num_classes=data[\"unified_class\"].nunique()).to(device)\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Инициализируем функцию потерь (loss/criterion), а так же оптимизатор, который будет регулировать обновление весов нашей модели\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Переменные для визуализации метрик и функции потерь\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Для удобства оценивать качество модели будем той же метрику, что на лидерборде - F1 score\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# Определим, сколько раз мы пройдёмся по всему датасету, прежде, чем закончим обучение модели и выберем лучшую версию\n",
    "num_epochs = 50\n",
    "\n",
    "# Шаговое уменьшение (StepLR)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Каждые 5 эпох уменьшать lr в 10 раз\n",
    "\n",
    "# Напишем свой train_loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_true = []\n",
    "    train_pred = []\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model((inputs, labels))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        # train_true.extend(labels.cpu().numpy())\n",
    "        # train_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    # train_f1 = f1_score(train_true, train_pred, average='macro')\n",
    "    # train_losses.append(running_loss / len(train_dataloader))\n",
    "    # train_f1_scores.append(train_f1)\n",
    "\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "\n",
    "    # валидационный цикл, когда мы оцениваем качество работы модели на отложенной выборке\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model((inputs, labels))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            val_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(val_true, val_pred, average='macro')\n",
    "    val_losses.append(val_running_loss / len(valid_dataloader))\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # если получившаяся модель лучше предыдущей, сохраним чекпоинт\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'New best model saved with F1: {best_val_f1:.4f}')\n",
    "\n",
    "\n",
    "    # выведем в консоль получившиеся результаты на отдельной эпохе\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "        #   f'Train Loss: {train_losses[-1]:.4f}, Train F1: {train_f1:.4f}, '\n",
    "          f'Val Loss: {val_losses[-1]:.4f}, Val F1: {val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_62214.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_6221...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_91562.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_9156...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_44104.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_4410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_79943.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_7994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_79847.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_7984...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>image_58364.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_5836...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>image_51853.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_5185...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>image_44601.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_4460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>image_08599.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_0859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>image_52486.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_5248...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  label                                         image_name\n",
       "0    image_62214.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_6221...\n",
       "1    image_91562.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_9156...\n",
       "2    image_44104.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_4410...\n",
       "3    image_79943.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_7994...\n",
       "4    image_79847.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_7984...\n",
       "..               ...    ...                                                ...\n",
       "595  image_58364.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_5836...\n",
       "596  image_51853.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_5185...\n",
       "597  image_44601.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_4460...\n",
       "598  image_08599.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_0859...\n",
       "599  image_52486.jpg    NaN  D:\\data science\\T1_hol\\dataset\\test/image_5248...\n",
       "\n",
       "[600 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(r\"D:\\data science\\T1_hol\\dataset\\annotations\\sample_submission.csv\")\n",
    "sample['image_name'] = r'D:\\data science\\T1_hol\\dataset\\test/'+sample['name']\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_18884\\1203183801.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4439bafe0aed4ee9a7007c4c2c2579b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>image_name</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_62214.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_6221...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_91562.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_9156...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_44104.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_4410...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_79943.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_7994...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_79847.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_7984...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>image_58364.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_5836...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>image_51853.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_5185...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>image_44601.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_4460...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>image_08599.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_0859...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>image_52486.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\data science\\T1_hol\\dataset\\test/image_5248...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  label  \\\n",
       "0    image_62214.jpg    NaN   \n",
       "1    image_91562.jpg    NaN   \n",
       "2    image_44104.jpg    NaN   \n",
       "3    image_79943.jpg    NaN   \n",
       "4    image_79847.jpg    NaN   \n",
       "..               ...    ...   \n",
       "595  image_58364.jpg    NaN   \n",
       "596  image_51853.jpg    NaN   \n",
       "597  image_44601.jpg    NaN   \n",
       "598  image_08599.jpg    NaN   \n",
       "599  image_52486.jpg    NaN   \n",
       "\n",
       "                                            image_name  predicted_class  \n",
       "0    D:\\data science\\T1_hol\\dataset\\test/image_6221...                2  \n",
       "1    D:\\data science\\T1_hol\\dataset\\test/image_9156...                2  \n",
       "2    D:\\data science\\T1_hol\\dataset\\test/image_4410...                2  \n",
       "3    D:\\data science\\T1_hol\\dataset\\test/image_7994...                2  \n",
       "4    D:\\data science\\T1_hol\\dataset\\test/image_7984...                2  \n",
       "..                                                 ...              ...  \n",
       "595  D:\\data science\\T1_hol\\dataset\\test/image_5836...                1  \n",
       "596  D:\\data science\\T1_hol\\dataset\\test/image_5185...                1  \n",
       "597  D:\\data science\\T1_hol\\dataset\\test/image_4460...                1  \n",
       "598  D:\\data science\\T1_hol\\dataset\\test/image_0859...                1  \n",
       "599  D:\\data science\\T1_hol\\dataset\\test/image_5248...                1  \n",
       "\n",
       "[600 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        return image, image_path\n",
    "\n",
    "\n",
    "# Тут важно не ошибиться и не использовать тренировочные трансформы\n",
    "infer_transform = tt.Compose([\n",
    "    tt.Resize((224, 224)),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Найдем все тестовые картинки\n",
    "test_image_paths = sample.image_name.tolist()\n",
    "\n",
    "infer_dataset = InferenceDataset(test_image_paths, transforms=infer_transform)\n",
    "infer_dataloader = DataLoader(infer_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# Инициализируем нашу модель и загрузим в неё лучшие после эксперимента веса\n",
    "# model = ResNet18(num_classes=data[\"unified_class\"].nunique()).to(device)\n",
    "# model = EfficientNet(num_classes=data[\"unified_class\"].nunique()).to(device)\n",
    "\n",
    "best_model_path = r'D:\\data science\\T1_hol\\best_model.pth'\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Не забудем перевести модель в режим предсказания, а не обучения.\n",
    "model.eval()\n",
    "\n",
    "# Для ускорения инференса будем подавать в модель картинки батчами (по несколько картинок за раз) и сохраним предсказанные метки классов.\n",
    "results = []\n",
    "for images, image_names in tqdm(infer_dataloader):\n",
    "    images = images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model((images, None)) #для не хагина\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        # outputs = model(images) # для хагина\n",
    "        # preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        results.append(preds[0])\n",
    "\n",
    "\n",
    "# Для удобства объединим все пары \"имя файла - предсказанный класс\" в датафрейм (таблицу) с колонками image_name, predicted_class\n",
    "sample['predicted_class'] = results\n",
    "\n",
    "# Вывод DataFrame\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'predicted_class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le1\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'predicted_class'"
     ]
    }
   ],
   "source": [
    "sample['label'] = le1.inverse_transform(sample['predicted_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.drop(['image_name','predicted_class'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_62214.jpg</td>\n",
       "      <td>rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_91562.jpg</td>\n",
       "      <td>rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_44104.jpg</td>\n",
       "      <td>rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_79943.jpg</td>\n",
       "      <td>rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_79847.jpg</td>\n",
       "      <td>rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>image_58364.jpg</td>\n",
       "      <td>fern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>image_51853.jpg</td>\n",
       "      <td>fern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>image_44601.jpg</td>\n",
       "      <td>fern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>image_08599.jpg</td>\n",
       "      <td>fern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>image_52486.jpg</td>\n",
       "      <td>fern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name label\n",
       "0    image_62214.jpg  rose\n",
       "1    image_91562.jpg  rose\n",
       "2    image_44104.jpg  rose\n",
       "3    image_79943.jpg  rose\n",
       "4    image_79847.jpg  rose\n",
       "..               ...   ...\n",
       "595  image_58364.jpg  fern\n",
       "596  image_51853.jpg  fern\n",
       "597  image_44601.jpg  fern\n",
       "598  image_08599.jpg  fern\n",
       "599  image_52486.jpg  fern\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним результат предсказаний в файл и всё! Можно отправлять!\n",
    "sample.to_csv(\"otv_l.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
